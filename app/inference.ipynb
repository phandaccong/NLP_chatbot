{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import math\n",
    "import pandas as pd\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from gensim.models import Word2Vec\n",
    "import json\n",
    "max_token = 200\n",
    "embedding_dims = 300\n",
    "\n",
    "tokenizer = Tokenizer.from_file(r'../file_vocab.json')\n",
    "with open(r'..\\BM250_retrival_Gomoe.pkl' , 'rb' ) as f:\n",
    "  retrival_gomoe = pickle.load(f)\n",
    "with open(r'../BM25O_Retrival_clinc.pkl' , 'rb') as f:\n",
    "  retrival_clinc = pickle.load(f)\n",
    "word2vec_clinc = Word2Vec.load(r'../Word2vec_cls_clinc.model')\n",
    "word2vec_gomoe = Word2Vec.load(r'../Word2vec_cls_gomoe.model')\n",
    "with open(r'../copus_clinc.json' , 'r') as f:\n",
    "  copus_clinc = json.load(f)\n",
    "with open(r'../copus_gomoe.json' , 'r') as f:\n",
    "  copus_gomoe = json.load(f)\n",
    "#load model\n",
    "\n",
    "\n",
    "def read_file_retrival_clinc(text):\n",
    "  clinc_re = retrival_clinc.get_scores(tokenizer.encode(text).tokens)\n",
    "  top_indices = sorted(range(len(clinc_re)), key=lambda i: clinc_re[i], reverse=True)[: 9]\n",
    "  dict_cls = {}\n",
    "  for inx in top_indices:\n",
    "    g = copus_clinc[str(inx)]\n",
    "    dict_cls[g] = dict_cls.get(g , 0) + 1\n",
    "  clas = max(dict_cls, key=dict_cls.get)\n",
    "  # print('Max class clinc', clas)\n",
    "  return word2vec_clinc.wv[clas]\n",
    "\n",
    "# load gomoe -> vector\n",
    "def read_file_retrival_gomoe(text):\n",
    "  gomoe_re = retrival_gomoe.get_scores(tokenizer.encode(text).tokens)\n",
    "  top_indices = sorted(range(len(gomoe_re)), key=lambda i: gomoe_re[i], reverse=True)[: 9]\n",
    "  dict_cls = {}\n",
    "  for inx in top_indices:\n",
    "    g = copus_gomoe[str(inx)]\n",
    "    dict_cls[g] = dict_cls.get(g , 0) + 1\n",
    "  clas = max(dict_cls, key=dict_cls.get)\n",
    "  # print('Max class gomoe', clas)\n",
    "  return word2vec_gomoe.wv[clas]\n",
    "\n",
    "\n",
    "\n",
    "pad = tokenizer.token_to_id(\"[PAD]\")\n",
    "def trun_pad_in(sequen):\n",
    "  # print('trun_pad_in' , sequen)\n",
    "  sequen = tokenizer.encode(str(sequen))\n",
    "  text_ids = sequen.ids[:max_token-1]\n",
    "  leng = max_token - len(text_ids)\n",
    "  text_ids += [pad] * leng\n",
    "  mask = np.where(np.array(text_ids) == 1 , 0 , 1)\n",
    "\n",
    "  return torch.tensor(text_ids, dtype = torch.long), torch.tensor(mask , dtype = torch.long)\n",
    "\n",
    "\n",
    "\n",
    "checkpoint = torch.load(r'../best_model.pth', weights_only=False)\n",
    "model = checkpoint['Model']\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "def autoregressive_predict(\n",
    "    model,\n",
    "    text,\n",
    "    max_token = 200,\n",
    "    embedding_dims = embedding_dims, max_len = 200):\n",
    "\n",
    "  model.eval()\n",
    "\n",
    "  token_ids , mask_in = trun_pad_in(text)\n",
    "  mask_in = mask_in.unsqueeze(0).type(torch.bool)\n",
    "  token_ids = token_ids.unsqueeze(1)\n",
    "  x_retrival_gome = torch.tensor(read_file_retrival_clinc(text)).unsqueeze(0).unsqueeze(0).repeat( 1 , max_token , 1 )\n",
    "  x_retrival_clinc = torch.tensor(read_file_retrival_gomoe(text)).unsqueeze(0).unsqueeze(0).repeat( 1 , max_token , 1 )\n",
    "\n",
    "  print('Shape x_retrival_gome', x_retrival_gome.shape)\n",
    "  print('Shape x_retrival_clinc', x_retrival_clinc.shape)\n",
    "\n",
    "  x_emb_retrival = torch.cat((x_retrival_gome ,x_retrival_clinc), dim = -1)\n",
    "\n",
    "\n",
    "  x_emb = model.embedding_pos(token_ids)\n",
    "  print('Shape x_emb_retrival', x_emb_retrival.shape)\n",
    "  print('Shape x_emb', x_emb.shape)\n",
    "\n",
    "  print('shape mask' , mask_in.shape)\n",
    "\n",
    "  sos_token_id = tokenizer.token_to_id(\"[SOS]\")\n",
    "  eos_token_id = tokenizer.token_to_id(\"[END]\")\n",
    "  tgt_out = torch.tensor([[eos_token_id]])\n",
    "  tgt_out = model.embedding_pos(tgt_out)\n",
    "  print('tgt_out' , tgt_out.shape)\n",
    "\n",
    "  x_emb = x_emb.permute(1, 0, 2)\n",
    "  tgt_out = tgt_out.permute(1, 0, 2)\n",
    "  x_emb_retrival = x_emb_retrival.permute(1, 0, 2)\n",
    "  with torch.no_grad():\n",
    "    for _ in range(max_len):\n",
    "      output = model(\n",
    "                      x_emb_retrival = x_emb_retrival,\n",
    "                      x_embedding = x_emb,\n",
    "                      tgt_out = tgt_out,\n",
    "                      mask_ids = mask_in\n",
    "                ) # -> (S, N , E )\n",
    "      print('out_put' , output.shape)\n",
    "      return output\n",
    "\n",
    "      next_token_logits = output[-1, 0, :]\n",
    "      next_token_id = next_token_logits.argmax(dim = -1).item()\n",
    "      tgt_out = torch.cat([tgt_out, torch.tensor([[next_token_id]])], dim=1)\n",
    "      if next_token_id == eos_token_id:\n",
    "        break\n",
    "    result_ids = tgt_out.squeeze(0).tolist()[1:]\n",
    "    result_text = tokenizer.decode(result_ids)\n",
    "    return result_text\n",
    "\n",
    "input_text = \"nhập câu test của bạn ở đây\"\n",
    "linh = autoregressive_predict(model, input_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
