{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 932
        },
        "collapsed": true,
        "id": "UbnbjYEL1MeB",
        "outputId": "dd9fabcc-2a98-46f1-a4da-7996faddead2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: numpy 1.24.4\n",
            "Uninstalling numpy-1.24.4:\n",
            "  Successfully uninstalled numpy-1.24.4\n",
            "Found existing installation: scipy 1.11.4\n",
            "Uninstalling scipy-1.11.4:\n",
            "  Successfully uninstalled scipy-1.11.4\n",
            "Found existing installation: gensim 4.3.2\n",
            "Uninstalling gensim-4.3.2:\n",
            "  Successfully uninstalled gensim-4.3.2\n",
            "Collecting numpy==1.24.4\n",
            "  Downloading numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting scipy==1.11.4\n",
            "  Downloading scipy-1.11.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gensim==4.3.2\n",
            "  Downloading gensim-4.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.3 kB)\n",
            "Collecting smart-open>=1.8.1 (from gensim==4.3.2)\n",
            "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting wrapt (from smart-open>=1.8.1->gensim==4.3.2)\n",
            "  Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Downloading numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m186.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.11.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m269.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gensim-4.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m176.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m211.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m315.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, numpy, smart-open, scipy, gensim\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 7.1.0\n",
            "    Uninstalling smart-open-7.1.0:\n",
            "      Successfully uninstalled smart-open-7.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.4 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.2 numpy-1.24.4 scipy-1.11.4 smart-open-7.1.0 wrapt-1.17.2\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "993a9f5c221c499c879d7b2ab2012048",
              "pip_warning": {
                "packages": [
                  "gensim",
                  "numpy",
                  "smart_open",
                  "wrapt"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip uninstall -y numpy scipy gensim\n",
        "!pip install numpy==1.24.4 scipy==1.11.4 gensim==4.3.2 --force-reinstall --no-cache-dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2ftDFOrJjrJ"
      },
      "source": [
        "# Mo dau PATH\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2L9BZx03hCF"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "\n",
        "import pandas as pd\n",
        "data_gome = pd.read_csv('/content/drive/MyDrive/data-raw/processing/data_new.csv')\n",
        "data_clinc = pd.read_csv('/content/drive/MyDrive/data-raw/processing/data_yd_clinc150.csv')\n",
        "data_daily = pd.read_csv('/content/drive/MyDrive/data-raw/processing/data_daily_dialog.csv')\n",
        "data_multi = pd.read_csv('/content/drive/MyDrive/data-raw/processing/data_MultiWoZ.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unRm4Pz84YiJ"
      },
      "source": [
        "Tổng hop thu tu hop ly theo nhan thuc cua bộ\n",
        "não\n",
        "\n",
        "Giai đoạn\n",
        "\n",
        "1. Cảm nhận\n",
        "(Perception) ->           ***  GoEmotions***\n",
        "\n",
        "2. Hiểu ý định (Intent\n",
        "Recognition) ->            *** CLINC150***\n",
        "\n",
        "3. Hiểu ngữ cảnh\n",
        "(Context Processing) ->    ***MultiWoz, DailyDialog, Persona-Chat***\n",
        "\n",
        "4. Sinh phản hồi\n",
        "(Response Generation) ->   ***Tập hội thoại đầy đủ***\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoxLH1aq4XcN",
        "outputId": "a54c2753-7285-4f16-94ac-df419d70ddd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rank-bm25 in /usr/local/lib/python3.11/dist-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rank-bm25) (1.24.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install rank-bm25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozF76RUw8Deo"
      },
      "source": [
        "# Làm sạch data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3eSz_wR5ORJ"
      },
      "outputs": [],
      "source": [
        "data_gome['clean_text'].isnull().sum()\n",
        "data_clinc.isnull().sum()\n",
        "data_daily['text'].isnull().sum()\n",
        "data_daily.dropna(axis = 0 , inplace= True)\n",
        "data_multi.isnull().sum()\n",
        "data_gome = data_gome.dropna(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3O41ZhS_rDP"
      },
      "outputs": [],
      "source": [
        "data_list = data_gome['clean_text'].to_list() + data_clinc['text'].to_list() + data_daily['text'].to_list() +\\\n",
        "            data_multi['text'].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbsEqH4R8k9s",
        "outputId": "633a4afe-206a-42c9-aabd-f30d89c66bfe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "503421"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(data_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgnsv8kdBFJi",
        "outputId": "a439467e-a944-44cb-aabb-e282887ba834"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "703"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max([len(x) for x in data_list])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuQ9jBCw0y8T"
      },
      "source": [
        "# Vocab-tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkGHF2z3-XIM"
      },
      "outputs": [],
      "source": [
        "# build vocab -  copus\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(BPE())\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "train = BpeTrainer(\n",
        "    special_tokens=[\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\" , \"[END]\" , \"[SOS]\"]\n",
        ")\n",
        "tokenizer.train_from_iterator(data_list, train )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1_anWkvjlbT"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FK8GQH0x-UH"
      },
      "outputs": [],
      "source": [
        "data_mul_bot = data_multi[data_multi['speaker'] == 0]\n",
        "data_mul_user = data_multi[data_multi['speaker'] == 1]\n",
        "\n",
        "emotion_columns = [col for col in data_gome.columns if col not in ['clean_text', \"text\"]]\n",
        "data_gome[\"cls\"]  = data_gome[emotion_columns].apply(lambda row: [col for col in emotion_columns if row[col] == 1 ]  , axis=1)\n",
        "data_gome[\"cls\"] = data_gome[\"cls\"].apply(lambda x: x if x else [\"normal\"])\n",
        "data_gome['cls'] = data_gome['cls'].apply(lambda x: ' '.join(x))\n",
        "data_gome_values = data_gome[['clean_text','cls']].values\n",
        "data_copus_clinc = data_clinc.values\n",
        "\n",
        "\n",
        "\n",
        "copus_clinc = [ tokenizer.encode(str(x)).tokens + [e] for x , e in data_copus_clinc ]\n",
        "copus_go_clinc = {idx : copus_clinc[idx][-1] for idx in range(len(copus_clinc))}\n",
        "data_Wv = [[x] for x in data_clinc['cls'].unique()]\n",
        "\n",
        "copus_gomoe = [tokenizer.encode(str(x)).tokens + [e] for x , e in data_gome_values]\n",
        "copus_go = {idx : copus_gomoe[idx][-1] for idx in range(len(copus_gomoe))}\n",
        "cls_gomoe = [[x]for x in data_gome[\"cls\"].unique()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHylUkijlPoo",
        "outputId": "7ada2bab-2742-4f00-f3f7-7f19ea81444a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(56776, 2)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_mul_bot.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0p4-9jc7wfBv"
      },
      "source": [
        "#  Lưu model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piHXGXjOGwx4",
        "outputId": "132748d9-bc93-476f-a065-3cdfe49b4fbf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{False, True}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "set([x[0] == 'amusement annoyance' for x in cls_gomoe])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hV698E7H3mx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDgkhjYf6kTD"
      },
      "outputs": [],
      "source": [
        "bm = BM25Okapi(copus_clinc)\n",
        "with open('/content/drive/MyDrive/data-raw/processing/BM25O_Retrival_clinc.pkl' , 'wb') as f:\n",
        "  pickle.dump(bm, f)\n",
        "bm1 = BM25Okapi(copus_gomoe)\n",
        "with open('/content/drive/MyDrive/data-raw/processing/BM250_retrival_Gomoe.pkl' , 'wb') as f:\n",
        "  pickle.dump(bm1, f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6-b_giO4SlJ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "with open('/content/drive/MyDrive/data-raw/processing/copus_gomoe.json', 'w') as f:\n",
        "    json.dump(copus_go, f)\n",
        "with open('/content/drive/MyDrive/data-raw/processing/copus_clinc.json', 'w') as f:\n",
        "    json.dump(copus_go_clinc, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zc7zEo2iIB7E"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec,  KeyedVectors\n",
        "e_label = Word2Vec(cls_gomoe, vector_size=32, window=1, min_count=1, workers=4)\n",
        "e_label.save('/content/drive/MyDrive/data-raw/processing/Word2vec_cls_gomoe.model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jbk4LPTlkebZ"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec,  KeyedVectors\n",
        "e_label = Word2Vec(cls_gomoe, vector_size=32, window=1, min_count=1, workers=4)\n",
        "e_label.save('/content/drive/MyDrive/data-raw/processing/Word2vec_cls_gomoe.model')\n",
        "\n",
        "\n",
        "e_label_clin  = Word2Vec(data_Wv, vector_size=32, window=1, min_count=1, workers=4)\n",
        "e_label_clin.save('/content/drive/MyDrive/data-raw/processing/Word2vec_cls_clinc.model' )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ejsg4xrhrxi"
      },
      "outputs": [],
      "source": [
        "word2vec_clinc = Word2Vec.load('/content/drive/MyDrive/data-raw/processing/Word2vec_cls_clinc.model')\n",
        "word2vec_gomoe = Word2Vec.load('/content/drive/MyDrive/data-raw/processing/Word2vec_cls_gomoe.model')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnlX_-dqHZo2"
      },
      "outputs": [],
      "source": [
        "word2vec_gomoe.wv['amusement annoyance']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzrVdDGEepAt"
      },
      "outputs": [],
      "source": [
        "max([len(x) for x in data_mul_bot['text']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1l16yxETIpj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojWtPm5nloRj"
      },
      "source": [
        "# DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w10Dd0JKSZwT"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8HVgFVsxLFF"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "max_token = 200\n",
        "import numpy\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import torch\n",
        "from rank_bm25 import BM25Okapi\n",
        "from gensim.models import Word2Vec\n",
        "import json\n",
        "# copus_clinc = [ tokenizer.encode(str(x)).tokens + [e] for x , e in data_copus_clinc ]\n",
        "# data_Wv = [[x] for x in data_clinc['cls'].unique()]\n",
        "\n",
        "\n",
        "data_mul_bot = data_multi[data_multi['speaker'] == 0]\n",
        "data_mul_user = data_multi[data_multi['speaker'] == 1]\n",
        "\n",
        "emotion_columns = [col for col in data_gome.columns if col not in ['clean_text', \"text\"]]\n",
        "data_gome[\"cls\"]  = data_gome[emotion_columns].apply(lambda row: [col for col in emotion_columns if row[col] == 1 ]  , axis=1)\n",
        "data_gome[\"cls\"] = data_gome[\"cls\"].apply(lambda x: x if x else [\"normal\"])\n",
        "data_gome['cls'] = data_gome['cls'].apply(lambda x: ' '.join(x))\n",
        "data_gome_values = data_gome[['clean_text','cls']].values\n",
        "data_copus_clinc = data_clinc.values\n",
        "\n",
        "\n",
        "\n",
        "copus_clinc = [ tokenizer.encode(str(x)).tokens + [e] for x , e in data_copus_clinc ]\n",
        "copus_go_clinc = {idx : copus_clinc[idx][-1] for idx in range(len(copus_clinc))}\n",
        "data_Wv = [[x] for x in data_clinc['cls'].unique()]\n",
        "\n",
        "copus_gomoe = [tokenizer.encode(str(x)).tokens + [e] for x , e in data_gome_values]\n",
        "copus_go = {idx : copus_gomoe[idx][-1] for idx in range(len(copus_gomoe))}\n",
        "cls_gomoe = [[x]for x in data_gome[\"cls\"].unique()]\n",
        "\n",
        "\n",
        "np.random.seed(31)\n",
        "default_vector = np.random.uniform(-1, 1, 64)\n",
        "# load_file trước\n",
        "with open('/content/drive/MyDrive/data-raw/processing/BM250_retrival_Gomoe.pkl' , 'rb' ) as f:\n",
        "  retrival_gomoe = pickle.load(f)\n",
        "with open('/content/drive/MyDrive/data-raw/processing/BM25O_Retrival_clinc.pkl' , 'rb') as f:\n",
        "  retrival_clinc = pickle.load(f)\n",
        "word2vec_clinc = Word2Vec.load('/content/drive/MyDrive/data-raw/processing/Word2vec_cls_clinc.model')\n",
        "word2vec_gomoe = Word2Vec.load('/content/drive/MyDrive/data-raw/processing/Word2vec_cls_gomoe.model')\n",
        "with open('/content/drive/MyDrive/data-raw/processing/copus_clinc.json' , 'r') as f:\n",
        "  copus_clinc = json.load(f)\n",
        "with open('/content/drive/MyDrive/data-raw/processing/copus_gomoe.json' , 'r') as f:\n",
        "  copus_gomoe = json.load(f)\n",
        "\n",
        "\n",
        "#load -> vector\n",
        "def read_file_retrival_clinc(text):\n",
        "  clinc_re = retrival_clinc.get_scores(tokenizer.encode(text).tokens)\n",
        "  top_indices = sorted(range(len(clinc_re)), key=lambda i: clinc_re[i], reverse=True)[: 9]\n",
        "  dict_cls = {}\n",
        "  for inx in top_indices:\n",
        "    g = copus_clinc[str(inx)]\n",
        "    dict_cls[g] = dict_cls.get(g , 0) + 1\n",
        "  clas = max(dict_cls, key=dict_cls.get)\n",
        "  # print('Max class clinc', clas)\n",
        "  return word2vec_clinc.wv[clas]\n",
        "\n",
        "\n",
        "# load gomoe -> vector\n",
        "def read_file_retrival_gomoe(text):\n",
        "  gomoe_re = retrival_gomoe.get_scores(tokenizer.encode(text).tokens)\n",
        "  top_indices = sorted(range(len(gomoe_re)), key=lambda i: gomoe_re[i], reverse=True)[: 9]\n",
        "  dict_cls = {}\n",
        "  for inx in top_indices:\n",
        "    g = copus_gomoe[str(inx)]\n",
        "    dict_cls[g] = dict_cls.get(g , 0) + 1\n",
        "\n",
        "  clas = max(dict_cls, key=dict_cls.get)\n",
        "  # print('Max class gomoe', clas)\n",
        "  return word2vec_gomoe.wv[clas]\n",
        "\n",
        "# truncate_padding\n",
        "pad = tokenizer.token_to_id(\"[PAD]\")\n",
        "def trun_pad_in(sequen):\n",
        "  # print('trun_pad_in' , sequen)\n",
        "  sequen = tokenizer.encode(str(sequen))\n",
        "  text_ids = sequen.ids[:max_token-1]\n",
        "  leng = max_token - len(text_ids)\n",
        "  text_ids += [pad] * leng\n",
        "  mask = np.where(np.array(text_ids) == 1 , 0 , 1)\n",
        "\n",
        "  return text_ids, mask\n",
        "\n",
        "sos = tokenizer.token_to_id(\"[SOS]\")\n",
        "end = tokenizer.token_to_id(\"[END]\")\n",
        "\n",
        "def trun_pad_out(sequence):\n",
        "  # print('trun_pad_out' , sequence)\n",
        "  tgt = [sos] + tokenizer.encode(str(sequence)).ids + [end]\n",
        "  tgt_out = tgt[1:]\n",
        "  tgt_out = tgt_out[: max_token-1]\n",
        "  tgt_out = tgt_out + [pad] * (max_token - len(tgt_out))\n",
        "\n",
        "  tgt_in = tgt[:-1]\n",
        "  tgt_in = tgt_in[: max_token-1]\n",
        "  tgt_in = tgt_in + [pad] * (max_token - len(tgt_in))\n",
        "  mask_tgt_in = np.where(np.array(tgt_in) == 1 , 0 , 1)\n",
        "  return tgt_out , tgt_in , mask_tgt_in\n",
        "\n",
        "\n",
        "#class loder\n",
        "class DatasetGo(Dataset):\n",
        "  def __init__( self, data_in , data_ids_output):\n",
        "    self.data_in = data_in\n",
        "    self.data_out = data_ids_output\n",
        "  def __len__(self):\n",
        "    return len(self.data_in)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    text = self.data_in[idx]\n",
        "    text_out = self.data_out[idx]\n",
        "\n",
        "    text_ids , text_mask = trun_pad_in(text)\n",
        "    text_re_gome = read_file_retrival_gomoe(text)\n",
        "    text_re_clinc = read_file_retrival_clinc(text)\n",
        "    tgt_out , tgt_in , mask_tgt_in = trun_pad_out(text_out)\n",
        "    #print(f\"Sample {idx}: text_ids={text_ids}, tgt_out={tgt_out}, tgt_in={tgt_in}\")\n",
        "    #print('text_re_gomoe', text_re_gome.shape)\n",
        "    return (\n",
        "        torch.tensor(text_ids , dtype = torch.long),\n",
        "        torch.tensor(text_re_gome , dtype = torch.float32),\n",
        "        torch.tensor(text_re_clinc , dtype = torch.float32),\n",
        "        torch.tensor(text_mask , dtype = torch.long),\n",
        "\n",
        "        torch.tensor(tgt_out , dtype = torch.long),\n",
        "        torch.tensor(tgt_in , dtype = torch.long),\n",
        "        torch.tensor(mask_tgt_in , dtype = torch.float32)\n",
        "            )\n",
        "\n",
        "\n",
        "x = data_mul_user['text'].to_list()[8000: 15000]\n",
        "y = data_mul_bot['text'].to_list()[8000:15000]\n",
        "\n",
        "Dat = DatasetGo(data_in = x , data_ids_output = y)\n",
        "\n",
        "dataloder = DataLoader(Dat ,\n",
        "                       batch_size = 64,\n",
        "                       shuffle = True ,\n",
        "                       num_workers= 2,\n",
        "                       prefetch_factor=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI13UV1b_afH"
      },
      "source": [
        "# Model chung"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDwEu7w5EHRB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import math\n",
        "# from kan import *\n",
        "\n",
        "# possition xác định vị trí các từ\n",
        "class Possiton(nn.Module):\n",
        "  def __init__(self, embedding_dim, dropout = 0.3 , max_len = 5000):\n",
        "    super(Possiton, self).__init__()\n",
        "    self.dropout = nn.Dropout(p = dropout)\n",
        "    position = torch.arange(max_len).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, embedding_dim, 2) * (-math.log(10000.0) / embedding_dim))\n",
        "    pe = torch.zeros(max_len, 1, embedding_dim)\n",
        "    pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "    self.register_buffer('pe', pe)\n",
        "  def forward(self, x):\n",
        "    x = x + self.pe[:x.size(1)].squeeze(1)\n",
        "    return self.dropout(x)\n",
        "\n",
        "class ModelMain(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      embedding_dims,\n",
        "      embeding_dims_retrival,\n",
        "      num_heads,\n",
        "      d_diff ,\n",
        "      num_layers,\n",
        "      dropout,\n",
        "      vocabulary\n",
        "      ):\n",
        "\n",
        "    super(ModelMain, self).__init__()\n",
        "\n",
        "    self.vocabulary = vocabulary\n",
        "    self.embedding_dims = embedding_dims\n",
        "    self.norm_emb = nn.RMSNorm(embedding_dims)\n",
        "    self.norm_ret = nn.RMSNorm(embeding_dims_retrival)\n",
        "\n",
        "    self.linear_encode = nn.LazyLinear(embedding_dims)\n",
        "\n",
        "    self.linear_encode_in = nn.Linear(embedding_dims , embedding_dims*2 )\n",
        "    self.linear_encode_in1 = nn.Linear(embedding_dims*2 , embedding_dims)\n",
        "\n",
        "\n",
        "\n",
        "# model encoder - layer *L:\n",
        "\n",
        "    self.model_encode = nn.TransformerEncoder( encoder_layer= nn.TransformerEncoderLayer( d_model = embedding_dims,\n",
        "                                                                                          nhead = num_heads,\n",
        "                                                                                          dim_feedforward= d_diff,\n",
        "                                                                                          dropout = dropout,\n",
        "                                                                                          activation = F.gelu ), num_layers = num_layers, norm = nn.RMSNorm(embedding_dims))\n",
        "\n",
        "# model experts 1:\n",
        "\n",
        "    self.norm_emb1 = nn.RMSNorm(embedding_dims)\n",
        "    self.norm_model1 = nn.RMSNorm(embedding_dims)\n",
        "    self.linear_encode_1 = nn.Linear(embedding_dims*2 , embedding_dims)\n",
        "    self.mode_trans_1 =  nn.TransformerEncoderLayer(d_model = embedding_dims, nhead= 2, dim_feedforward= embedding_dims*2, dropout= 0.3 , activation= F.gelu )\n",
        "\n",
        "# model 2:\n",
        "\n",
        "    self.linaer_text_cat2 = nn.LazyLinear(embedding_dims)\n",
        "    self.norm_emb2 = nn.RMSNorm(embedding_dims)\n",
        "    self.norm_model2 = nn.RMSNorm(embedding_dims)\n",
        "\n",
        "    self.linear_2 = nn.Linear(embedding_dims*2 , embedding_dims)\n",
        "\n",
        "    self.model_trans_2 = nn.TransformerEncoderLayer( d_model= embedding_dims, nhead =  2 , dim_feedforward= embedding_dims *2, dropout = 0.3, activation= F.gelu)\n",
        "\n",
        "\n",
        "# decode 4\n",
        "    self.norm_emb3 = nn.RMSNorm(embedding_dims)\n",
        "    self.norm_model3 = nn.RMSNorm(embedding_dims)\n",
        "\n",
        "    self.model_deocoder = nn.TransformerDecoder(\n",
        "        decoder_layer =  nn.TransformerDecoderLayer(\n",
        "                          d_model = embedding_dims,\n",
        "                          nhead = num_heads,\n",
        "                          dim_feedforward= d_diff,\n",
        "                          dropout = dropout,\n",
        "                          activation = F.gelu),\n",
        "        num_layers= num_layers , norm = nn.RMSNorm(embedding_dims)\n",
        "    )\n",
        "# Autoregressive9 256 , embedding_dims*2)\n",
        "    self.linear_autoregre1 = nn.Linear(embedding_dims ,embedding_dims + 256)\n",
        "    self.linear_autoregre2 = nn.Linear(embedding_dims + 256 , embedding_dims*2)\n",
        "    self.linear_autoregre = nn.Linear(embedding_dims*2 , vocabulary)\n",
        "# kan\n",
        "    # self.kan = KAN(\n",
        "    #     width = [embedding_dims , embedding_dims*2 , embedding_dims]\n",
        "    # )\n",
        "  def embedding_pos(self, x):\n",
        "    \"\"\"\n",
        "    Embedding + Position -> ( seq_length , batch_size , embedding_dims) ( N, S, E)\n",
        "    \"\"\"\n",
        "    embedding = nn.Embedding(self.vocabulary, self.embedding_dims)\n",
        "    position = Possiton(self.embedding_dims)\n",
        "    src = embedding(x)\n",
        "    src = position(src)\n",
        "    src = src.permute(1, 0, 2)\n",
        "    # print('embedding')\n",
        "    return src\n",
        "\n",
        "  def forward(self, x_emb_retrival , x_embedding ,tgt_out, tgt_mask_key = None,  mask_ids = None ):\n",
        "    \"\"\"\n",
        "    x_emb_retrival ( N , S , E)\n",
        "    x_embedding ( N , S , E )\n",
        "    \"\"\"\n",
        "    # input\n",
        "    x_norm_retri_encode = self.norm_ret(x_emb_retrival)\n",
        "    x_norm_emb_encode = self.norm_emb(x_embedding)\n",
        "    x_concat = torch.cat((x_norm_retri_encode , x_norm_emb_encode) , dim = -1)\n",
        "    #print(\"x_concat\", x_concat )\n",
        "    x_encode = self.linear_encode(x_concat)\n",
        "    x_encode = F.gelu(x_encode)\n",
        "    x_encode = self.linear_encode_in(x_encode)\n",
        "    x_encode = F.gelu(x_encode)\n",
        "    x_encode = self.linear_encode_in1(x_encode)\n",
        "    #print(\"shape \" , x_encode.shape)\n",
        "    # xử lý text model chung\n",
        "    encode_chung = self.model_encode(x_encode , src_key_padding_mask = mask_ids)\n",
        "    # print('encode_chung' , encode_chung.shape\n",
        "\n",
        "    #model 1\n",
        "    emb_1 = self.norm_emb1(x_encode)\n",
        "    model_norm_1 = self.norm_model1(encode_chung)\n",
        "    model_concat_1 = torch.cat((emb_1 , model_norm_1) , dim = -1)\n",
        "    model_1 = self.linear_encode_1(model_concat_1)\n",
        "    model_1 = self.mode_trans_1(model_1 , src_key_padding_mask = mask_ids)\n",
        "    #model 2\n",
        "\n",
        "    text_2 = self.linaer_text_cat2(x_concat)\n",
        "    emb_2 = self.norm_emb2(text_2)\n",
        "    model_norm_2 = self.norm_model2(model_1)\n",
        "\n",
        "    model_concat_2 = torch.cat((emb_2 , model_norm_2) , dim = -1)\n",
        "    model_2 = self.linear_2(model_concat_2)\n",
        "    model_2 = self.model_trans_2(model_2 , src_key_padding_mask = mask_ids)\n",
        "    #model encode\n",
        "  #  print(\"trước tgt_len\" , model_2.shape , \"tgt.size(1)\",tgt_out.size(0))\n",
        "    tgt_len = tgt_out.size(0)\n",
        "    tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_len)\n",
        "    model_3 = self.model_deocoder(tgt = tgt_out , memory = model_2 ,tgt_mask = tgt_mask , tgt_is_causal = True, tgt_key_padding_mask =tgt_mask_key)\n",
        "# linear FFC\n",
        "    linear_1 = self.linear_autoregre1(model_3)\n",
        "    linear_1 = F.gelu(linear_1)\n",
        "    linear_1 = nn.Dropout(0.3)(linear_1)\n",
        "    linear_2 = self.linear_autoregre2(linear_1)\n",
        "    linear_2 = F.gelu(linear_2)\n",
        "    linear_2 = nn.Dropout(0.3)(linear_2)\n",
        "    Final = self.linear_autoregre(linear_2)\n",
        "# linaer KAN\n",
        "\n",
        "\n",
        "    return Final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfA_ym8xu8HL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_MAz89sSLlG"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint( model, loss , optimizer):\n",
        "  file_name = '/content/drive/MyDrive/data-raw/processing/best_model.pth'\n",
        "  torch.save(\n",
        "      {\n",
        "          'Model' : model,\n",
        "          'model_state_dict': model.state_dict(),\n",
        "          'optimizer_state_dict': optimizer.state_dict(),\n",
        "          'loss': loss.item()\n",
        "      },\n",
        "      file_name\n",
        "  )\n",
        "  print('Saved checkpoint ------------')\n",
        "\n",
        "\n",
        "def load_checkpoint(model_ecoder, loss ):\n",
        "    file_path = '/content/drive/MyDrive/data-raw/processing/best_model.pth'\n",
        "\n",
        "    checkpoint = torch.load(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmowQs-yMXil"
      },
      "source": [
        "# trainning new start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "zeqFJemDRnuZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def trainning(DataLoader, epochs):\n",
        "  embedding_dims = 300\n",
        "  num_classes = 26\n",
        "  vocab_size = int(tokenizer.get_vocab_size())\n",
        "  num_layer_main = 10\n",
        "  num_heads = 10\n",
        "  dropout = 0.3\n",
        "  model_encoder = ModelMain(\n",
        "                              embedding_dims = embedding_dims,\n",
        "                              num_heads = num_heads,\n",
        "                              num_layers = num_layer_main,\n",
        "                              dropout = dropout,\n",
        "                              embeding_dims_retrival = 64,\n",
        "                              d_diff= 256,\n",
        "                              vocabulary = vocab_size)\n",
        "\n",
        "  model_encoder.train()\n",
        "  padding_token = tokenizer.token_to_id(\"[PAD]\")\n",
        "  cretion = nn.CrossEntropyLoss(ignore_index=padding_token , label_smoothing=0.1)\n",
        "  optimizer_main = torch.optim.Adam(model_encoder.parameters() , lr = 0.001)\n",
        "  scheduler_main = torch.optim.lr_scheduler.StepLR(optimizer_main, step_size=10, gamma=0.1)\n",
        "  best_model  = float(\"inf\")\n",
        "  for epoch in range(epochs):\n",
        "    loser = []\n",
        "\n",
        "\n",
        "    for i , (text_ids, text_re_gomoe , text_re_clinc, text_mask, tgt_out, tgt_in, mask_tgt_in) in enumerate(DataLoader):\n",
        "      \"\"\"\n",
        "      text_ids: data inphut text_raw\n",
        "      text_re_gomoe , text_re_clinc : text_gome và clinc\n",
        "      text_mask: mask text_ids\n",
        "      tgt_out: target output\n",
        "      tgt_in: target input\n",
        "      mask_tgt_in: mask target input\n",
        "      \"\"\"\n",
        "      with torch.no_grad():\n",
        "        embedding_text_input = model_encoder.embedding_pos(text_ids)\n",
        "        embedding_text_ouputin = model_encoder.embedding_pos(tgt_in)\n",
        "        text_mask = text_mask.type(torch.bool)\n",
        "        mask_tgt_in = mask_tgt_in.type(torch.bool)\n",
        "        #tgt_mask_key = tgt_mask_key.type(torch.bool)\n",
        "        # tạo sequense theo embeding\n",
        "\n",
        "        #print(f'Lan {i} , text_re_gome {text_re_gomoe.shape} embedding_text_input {embedding_text_input.shape} , embedding_text_ouputin {embedding_text_ouputin.shape}')\n",
        "       # print(f'text_mask {text_mask.shape} mask_tgt_in {mask_tgt_in.shape}')\n",
        "\n",
        "        sequen_retrival_gome = text_re_gomoe.unsqueeze(1).repeat(1 , max_token , 1).permute(1 ,0 , 2) #( N, S, E )\n",
        "        sequen_retrival_clinc = text_re_clinc.unsqueeze(1).repeat(1 , max_token , 1).permute(1, 0, 2) #( N, S, E )\n",
        "\n",
        "\n",
        "        # print('sequen_retrival_gome' , sequen_retrival_gome.shape)\n",
        "        # print('sequen_retrival_clinc' , sequen_retrival_clinc.shape)\n",
        "        # print('embedding_text_input' , embedding_text_input.shape)\n",
        "        # print('embedding_text_ouputin' , embedding_text_ouputin.shape)\n",
        "        # print(' shape mask_tgt_in' , mask_tgt_in.shape)\n",
        "        \"\"\"\n",
        "        sequen_retrival_gome torch.Size([32, 200, 64])\n",
        "  sequen_retrival_clinc torch.Size([32, 200, 64])\n",
        "        embedding_text_input torch.Size([200, 64, 128])\n",
        "  embedding_text_ouputin torch.Size([200, 64, 128])\n",
        "  vector_em_retri torch.Size([32, 200, 128])\n",
        "        \"\"\"\n",
        "\n",
        "        vector_em_retri = torch.cat((sequen_retrival_gome , sequen_retrival_clinc) ,dim = -1  )\n",
        "        #print('tgt_out' , tgt_out)\n",
        "\n",
        "\n",
        "        #embeding_final = torch.cat((embedding_text_input ,vector_em_retri) , dim = -1)\n",
        "        #print('embeding_final' , embeding_final.shape)\n",
        "\n",
        "  # model trainning\n",
        "\n",
        "        #print(text_mask.shape)\n",
        "\n",
        "      out = model_encoder(vector_em_retri,\n",
        "                          embedding_text_input,\n",
        "                          embedding_text_ouputin,\n",
        "                          mask_tgt_in,\n",
        "                          text_mask\n",
        "                          )\n",
        "\n",
        "      # MSE crossentropy out -> (S, N , E ) ( N , S)\n",
        "      #print('return' , out.shape , 'tgt_out', tgt_out.shape)\n",
        "\n",
        "      #converts_dim_out = tgt_out.permute(1, 0)\n",
        "      #print('tgt_out', tgt_out.shape)\n",
        "      out = out.permute(1 , 2, 0)\n",
        "      loss = cretion( out , tgt_out ) # model(N, E , S) - tgt(N, S)\n",
        "      optimizer_main.zero_grad()\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer_main.step()\n",
        "      loser.append(loss.item())\n",
        "\n",
        "      print(f'epoch {epoch}/ {epochs} batch {i}/ {len(DataLoader)} loss {loss.item()} ')\n",
        "\n",
        "      if loss.item() < best_model:\n",
        "        best_model = loss.item()\n",
        "        save_checkpoint(\n",
        "            loss = loss,\n",
        "            model = model_encoder,\n",
        "            optimizer = optimizer_main\n",
        "        )\n",
        "        print(f\"da luu model voi loss {loss.item()}\")\n",
        "\n",
        "    print('trung bình loss ' , sum(loser) / len(loser))\n",
        "m = trainning(dataloder,epochs = 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLXzkPMXzlrr"
      },
      "source": [
        "# Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eerfVoUoKOSY",
        "outputId": "3725da69-f1d5-4351-a2f5-b8b15efa0406"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "30000"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.get_vocab_size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUtZ6ie2z7ir",
        "outputId": "92bed321-b9c6-4f68-e416-1be9e1795575"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " load loss again  5.772792816162109\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0/2 batch 0/110 loss 5.708310604095459\n",
            "Saved checkpoint ------------\n",
            "Đã lưu model với loss 5.708310604095459\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 1/110 loss 5.710229873657227\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 2/110 loss 5.738039493560791\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 3/110 loss 5.572866439819336\n",
            "Saved checkpoint ------------\n",
            "Đã lưu model với loss 5.572866439819336\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 4/110 loss 5.750107765197754\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 5/110 loss 5.5857391357421875\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 6/110 loss 5.674737930297852\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 7/110 loss 5.5983991622924805\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 8/110 loss 5.5730204582214355\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 9/110 loss 5.509597301483154\n",
            "Saved checkpoint ------------\n",
            "Đã lưu model với loss 5.509597301483154\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 10/110 loss 5.801849842071533\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 11/110 loss 5.642212867736816\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 12/110 loss 5.7575836181640625\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 13/110 loss 5.703612327575684\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 14/110 loss 5.7913103103637695\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 15/110 loss 5.688014984130859\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 16/110 loss 5.580681800842285\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 17/110 loss 5.69261360168457\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 18/110 loss 5.543703556060791\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 19/110 loss 5.626020908355713\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 20/110 loss 5.704962730407715\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 21/110 loss 5.619792938232422\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 22/110 loss 5.581665992736816\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 23/110 loss 5.809356689453125\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 24/110 loss 5.585348606109619\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 25/110 loss 5.608369827270508\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 26/110 loss 5.770874977111816\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 27/110 loss 5.641003131866455\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 28/110 loss 5.600924491882324\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 29/110 loss 5.657394886016846\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 30/110 loss 5.583009243011475\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 31/110 loss 5.660758972167969\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 32/110 loss 5.673650741577148\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 33/110 loss 5.634237289428711\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 34/110 loss 5.739457607269287\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 35/110 loss 5.734250068664551\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 36/110 loss 5.628293991088867\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 37/110 loss 5.581313610076904\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 38/110 loss 5.63497257232666\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 39/110 loss 5.552954196929932\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 40/110 loss 5.700469493865967\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 41/110 loss 5.746406078338623\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 42/110 loss 5.65182638168335\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 43/110 loss 5.80064058303833\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 44/110 loss 5.728135108947754\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 45/110 loss 5.6062774658203125\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 46/110 loss 5.678825855255127\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 47/110 loss 5.75850248336792\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 48/110 loss 5.614658355712891\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 49/110 loss 5.61245059967041\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 50/110 loss 5.60823917388916\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 51/110 loss 5.634522914886475\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 52/110 loss 5.7094268798828125\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 53/110 loss 5.581401824951172\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 54/110 loss 5.666678428649902\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 55/110 loss 5.648750305175781\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 56/110 loss 5.538086891174316\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 57/110 loss 5.729111194610596\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 58/110 loss 5.712019443511963\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 59/110 loss 5.724825859069824\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n",
            "epoch 0/2 batch 60/110 loss 5.562450408935547\n",
            "trước tgt_len torch.Size([200, 64, 300]) tgt.size(1) 200\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Khởi tạo model và optimizer\n",
        "embedding_dims = 300\n",
        "num_classes = 26\n",
        "vocab_size = int(tokenizer.get_vocab_size())\n",
        "num_layer_main = 10\n",
        "num_heads = 10\n",
        "dropout = 0.3\n",
        "max_token = 200\n",
        "model_main = ModelMain(\n",
        "    embedding_dims=embedding_dims,\n",
        "    num_heads=num_heads,\n",
        "    num_layers=num_layer_main,\n",
        "    dropout=dropout,\n",
        "    embeding_dims_retrival=64,\n",
        "    d_diff=256,\n",
        "    vocabulary=vocab_size\n",
        ")\n",
        "\n",
        "optimizer_main = torch.optim.AdamW(model_main.parameters(), lr=3e-4, weight_decay=1e-4)\n",
        "\n",
        "# Định nghĩa hàm training\n",
        "def pre_training(Dataloder, epochs, optimizer_main, model_main):\n",
        "    # Load model\n",
        "    file_path = '/content/drive/MyDrive/data-raw/processing/best_model.pth'\n",
        "    model_load = torch.load(file_path, weights_only = False)\n",
        "    optimizer_main.load_state_dict(model_load['optimizer_state_dict'])\n",
        "    model_main.load_state_dict(model_load['model_state_dict'])\n",
        "\n",
        "    padding_token = tokenizer.token_to_id(\"[PAD]\")\n",
        "    cretion = nn.CrossEntropyLoss(ignore_index=padding_token, label_smoothing=0.05) # giảm label_smothing để model tập trung hơn vào việc tối ưu\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer_main, step_size=1, gamma=0.95)\n",
        "\n",
        "    model_main.train()\n",
        "    best_model = model_load['loss']\n",
        " # Đặt giá trị lớn để so sánh loss tốt hơn\n",
        "    print(' load loss again ' , model_load['loss'] )\n",
        "    for epoch in range(epochs):\n",
        "        loser = []\n",
        "        for i, (text_ids, text_re_gomoe, text_re_clinc, text_mask, tgt_out, tgt_in, mask_tgt_in) in enumerate(Dataloder):\n",
        "\n",
        "            with torch.no_grad():\n",
        "                embedding_text_input = model_main.embedding_pos(text_ids)\n",
        "                embedding_text_ouputin = model_main.embedding_pos(tgt_in)\n",
        "                text_mask = text_mask.type(torch.bool)\n",
        "                mask_tgt_in = mask_tgt_in.type(torch.bool)\n",
        "\n",
        "                # Kiểm tra max_token có tồn tại không\n",
        "\n",
        "                sequen_retrival_gome = text_re_gomoe.unsqueeze(1).repeat(1, max_token, 1).permute(1, 0, 2)\n",
        "                sequen_retrival_clinc = text_re_clinc.unsqueeze(1).repeat(1, max_token, 1).permute(1, 0, 2)\n",
        "                vector_em_retri = torch.cat((sequen_retrival_gome, sequen_retrival_clinc), dim=-1)\n",
        "\n",
        "\n",
        "            # Forward qua model\n",
        "            out = model_main(vector_em_retri, embedding_text_input, embedding_text_ouputin, mask_tgt_in, text_mask)\n",
        "\n",
        "\n",
        "            out = out.permute(1, 2, 0)\n",
        "\n",
        "            loss = cretion(out, tgt_out)\n",
        "\n",
        "            optimizer_main.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer_main.step()\n",
        "            scheduler.step()\n",
        "            loser.append(loss.item())\n",
        "\n",
        "            print(f'epoch {epoch}/{epochs} batch {i}/{len(Dataloder)} loss {loss.item()}')\n",
        "\n",
        "            # Lưu model nếu loss tốt hơn\n",
        "            if loss.item() < best_model:\n",
        "                best_model = loss.item()\n",
        "                save_checkpoint(\n",
        "                    loss=loss,\n",
        "                    model=model_main,\n",
        "                    optimizer=optimizer_main\n",
        "                )\n",
        "                print(f\"Đã lưu model với loss {loss.item()}\")\n",
        "\n",
        "        print('Trung bình loss:', sum(loser) / len(loser))\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "text_mask torch.Size([32, 200])\n",
        "embedding_text_ouputin torch.Size([200, 32, 300])\n",
        "mask_tgt_in torch.Size([32, 200])\n",
        "embedding_text_input torch.Size([200, 32, 300])\n",
        "vector_em_retri torch.Size([200, 32, 64])\n",
        "sequen_retrival_gome torch.Size([200, 32, 32])\n",
        "sequen_retrival_clinc torch.Size([200, 32, 32])\n",
        "trước tgt_len torch.Size([200, 32, 300]) tgt.size(1)\n",
        "\n",
        "out torch.Size([200, 32, 30000])\n",
        "out.permute torch.Size([32, 30000, 200])\n",
        "\"\"\"\n",
        "\n",
        "pre_training(dataloder , 2, optimizer_main, model_main )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Oad-oxoNDjZ"
      },
      "source": [
        "# Inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJLB3on6NC5C",
        "outputId": "7b60401e-4bc8-46ae-9dba-ff50f11848ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape x_retrival_gome torch.Size([1, 200, 32])\n",
            "Shape x_retrival_clinc torch.Size([1, 200, 32])\n",
            "Shape x_emb_retrival torch.Size([1, 200, 64])\n",
            "Shape x_emb torch.Size([1, 200, 300])\n",
            "shape mask torch.Size([1, 200])\n",
            "tgt_out torch.Size([1, 1, 300])\n",
            "out_put torch.Size([1, 1, 30000])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import math\n",
        "import pandas as pd\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from gensim.models import Word2Vec\n",
        "import json\n",
        "max_token = 200\n",
        "embedding_dims = 300\n",
        "\n",
        "tokenizer = Tokenizer.from_file('/content/drive/MyDrive/data-raw/processing/file_vocab.json')\n",
        "with open('/content/drive/MyDrive/data-raw/processing/BM250_retrival_Gomoe.pkl' , 'rb' ) as f:\n",
        "  retrival_gomoe = pickle.load(f)\n",
        "with open('/content/drive/MyDrive/data-raw/processing/BM25O_Retrival_clinc.pkl' , 'rb') as f:\n",
        "  retrival_clinc = pickle.load(f)\n",
        "word2vec_clinc = Word2Vec.load('/content/drive/MyDrive/data-raw/processing/Word2vec_cls_clinc.model')\n",
        "word2vec_gomoe = Word2Vec.load('/content/drive/MyDrive/data-raw/processing/Word2vec_cls_gomoe.model')\n",
        "with open('/content/drive/MyDrive/data-raw/processing/copus_clinc.json' , 'r') as f:\n",
        "  copus_clinc = json.load(f)\n",
        "with open('/content/drive/MyDrive/data-raw/processing/copus_gomoe.json' , 'r') as f:\n",
        "  copus_gomoe = json.load(f)\n",
        "#load model\n",
        "\n",
        "\n",
        "def read_file_retrival_clinc(text):\n",
        "  clinc_re = retrival_clinc.get_scores(tokenizer.encode(text).tokens)\n",
        "  top_indices = sorted(range(len(clinc_re)), key=lambda i: clinc_re[i], reverse=True)[: 9]\n",
        "  dict_cls = {}\n",
        "  for inx in top_indices:\n",
        "    g = copus_clinc[str(inx)]\n",
        "    dict_cls[g] = dict_cls.get(g , 0) + 1\n",
        "  clas = max(dict_cls, key=dict_cls.get)\n",
        "  # print('Max class clinc', clas)\n",
        "  return word2vec_clinc.wv[clas]\n",
        "\n",
        "# load gomoe -> vector\n",
        "def read_file_retrival_gomoe(text):\n",
        "  gomoe_re = retrival_gomoe.get_scores(tokenizer.encode(text).tokens)\n",
        "  top_indices = sorted(range(len(gomoe_re)), key=lambda i: gomoe_re[i], reverse=True)[: 9]\n",
        "  dict_cls = {}\n",
        "  for inx in top_indices:\n",
        "    g = copus_gomoe[str(inx)]\n",
        "    dict_cls[g] = dict_cls.get(g , 0) + 1\n",
        "  clas = max(dict_cls, key=dict_cls.get)\n",
        "  # print('Max class gomoe', clas)\n",
        "  return word2vec_gomoe.wv[clas]\n",
        "\n",
        "\n",
        "\n",
        "pad = tokenizer.token_to_id(\"[PAD]\")\n",
        "def trun_pad_in(sequen):\n",
        "  # print('trun_pad_in' , sequen)\n",
        "  sequen = tokenizer.encode(str(sequen))\n",
        "  text_ids = sequen.ids[:max_token-1]\n",
        "  leng = max_token - len(text_ids)\n",
        "  text_ids += [pad] * leng\n",
        "  mask = np.where(np.array(text_ids) == 1 , 0 , 1)\n",
        "\n",
        "  return torch.tensor(text_ids, dtype = torch.long), torch.tensor(mask , dtype = torch.long)\n",
        "\n",
        "\n",
        "\n",
        "checkpoint = torch.load('/content/drive/MyDrive/data-raw/processing/best_model.pth', weights_only=False)\n",
        "model = checkpoint['Model']\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "def autoregressive_predict(\n",
        "    model,\n",
        "    text,\n",
        "    max_token = 200,\n",
        "    embedding_dims = embedding_dims, max_len = 200):\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  token_ids , mask_in = trun_pad_in(text)\n",
        "  mask_in = mask_in.unsqueeze(0).type(torch.bool)\n",
        "  token_ids = token_ids.unsqueeze(1)\n",
        "  x_retrival_gome = torch.tensor(read_file_retrival_clinc(text)).unsqueeze(0).unsqueeze(0).repeat( 1 , max_token , 1 )\n",
        "  x_retrival_clinc = torch.tensor(read_file_retrival_gomoe(text)).unsqueeze(0).unsqueeze(0).repeat( 1 , max_token , 1 )\n",
        "\n",
        "  print('Shape x_retrival_gome', x_retrival_gome.shape)\n",
        "  print('Shape x_retrival_clinc', x_retrival_clinc.shape)\n",
        "\n",
        "  x_emb_retrival = torch.cat((x_retrival_gome ,x_retrival_clinc), dim = -1)\n",
        "\n",
        "\n",
        "  x_emb = model.embedding_pos(token_ids)\n",
        "  print('Shape x_emb_retrival', x_emb_retrival.shape)\n",
        "  print('Shape x_emb', x_emb.shape)\n",
        "\n",
        "  print('shape mask' , mask_in.shape)\n",
        "\n",
        "  sos_token_id = tokenizer.token_to_id(\"[SOS]\")\n",
        "  eos_token_id = tokenizer.token_to_id(\"[END]\")\n",
        "  tgt_out = torch.tensor([[eos_token_id]])\n",
        "  tgt_out = model.embedding_pos(tgt_out)\n",
        "  print('tgt_out' , tgt_out.shape)\n",
        "\n",
        "  x_emb = x_emb.permute(1, 0, 2)\n",
        "  tgt_out = tgt_out.permute(1, 0, 2)\n",
        "  x_emb_retrival = x_emb_retrival.permute(1, 0, 2)\n",
        "  with torch.no_grad():\n",
        "    for _ in range(max_len):\n",
        "      output = model(\n",
        "                      x_emb_retrival = x_emb_retrival,\n",
        "                      x_embedding = x_emb,\n",
        "                      tgt_out = tgt_out,\n",
        "                      mask_ids = mask_in\n",
        "                ) # -> (S, N , E )\n",
        "      print('out_put' , output.shape)\n",
        "      return output\n",
        "\n",
        "      next_token_logits = output[-1, 0, :]\n",
        "      next_token_id = next_token_logits.argmax(dim = -1).item()\n",
        "      tgt_out = torch.cat([tgt_out, torch.tensor([[next_token_id]])], dim=1)\n",
        "      if next_token_id == eos_token_id:\n",
        "        break\n",
        "    result_ids = tgt_out.squeeze(0).tolist()[1:]\n",
        "    result_text = tokenizer.decode(result_ids)\n",
        "    return result_text\n",
        "\n",
        "input_text = \"nhập câu test của bạn ở đây\"\n",
        "linh = autoregressive_predict(model, input_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "w2ftDFOrJjrJ",
        "ozF76RUw8Deo",
        "0p4-9jc7wfBv",
        "BmowQs-yMXil"
      ],
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
